---
title: "Assignment1"
subtitle: "Bayesian Data Analysis and Models of Behavior"
author: "Wenjie Tu"
date: "Spring Semester 2022"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

$~$

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang="us_en")
rm(list=ls())
```

### 1 Six Dice Game

$~$

#### 1(a) Theoretical expected payoff

$$
P(X=k)=\binom{n}{k}p^{k}(1-p)^{n-k}
$$

* $X$: the random variable that we want to model
* $n$: the total number of dice rolled in this game
* $k$: the number of dice landing with a face of 6
* $p$: the probability that a die lands with a face of 6

```{r}
## Initialize a 6x2 data frame
dat <- data.frame(matrix(nrow=6, ncol=3))
colnames(dat) <- c("Outcome", "Probability", "Payoff")

n <- nrow(dat)
for (k in 1:n) {
  dat[k, 1] <- k # the number dice showing 6 on the top face
  dat[k, 2] <- choose(n, k) * (1/6)^k * (5/6)^(n-k) # probability
  dat[k, 3] <- 0.5*(1+k) # payoff
}
```

```{r}
knitr::kable(dat, align="c", caption="Six dice game")
```

```{r}
## Expected payoff of playing this game
expected.payoff <- with(dat, Probability %*% Payoff)[1]; expected.payoff
```

We see that the expected payoff of playing this game is 0.833, which is smaller than the initial cost (i.e. 1) for playing such a game. Therefore, it is not a good idea to play this game.

$~$

#### 1(b) Monte Carlo simulation

One can repeatedly play this game for thousands of times. A Monte Carlo simulation can be conducted to model this procedure.

```{r}
## Set seed for reproducible results
set.seed(4321)

iterations <- 5000 # the number of iterations
cost <- 1          # the initial cost of playing the game one time

## Generate a data frame for plotting
d.payoff <- data.frame(matrix(nrow=iterations, ncol=3)) 
colnames(d.payoff) <- c("Iterations", "Payoff", "Expectation")

for (i in 1:iterations) {
  ## Roll 6 dice and store the outcome
  outcome <- sample(1:6, size=6, replace=TRUE)
  
  ## Compute the number of dice landing with 6 each time
  n.six <- sum(outcome==6)
  
  ## Store the iteration index in the first column
  d.payoff[i, 1] <- i
  
  ## Store the payoff for each iteration in the second column
  if (n.six==0) {
    d.payoff[i, 2] <- 0
  } else {
    d.payoff[i, 2] <- 0.5*(1+n.six)
  }
  
  ## Compute the expected payoff in the third column
  d.payoff[i, 3] <- mean(d.payoff[1:i, 2])
}
```


```{r, warning=FALSE}
## Load packages for generating plots
library(ggplot2)
library(gganimate)
library(gifski)
```


```{r}
ggplot(d.payoff, aes(x=Iterations, y=Expectation)) + 
  geom_line(color=6) + 
  geom_point(color=6) + 
  geom_hline(yintercept=expected.payoff, color=4, linetype="longdash") +
  geom_text(aes(0, expected.payoff, label="theoretical expected payoff", vjust=1.2, hjust=-0.5), color=4) +
  geom_hline(yintercept=cost, color=2, linetype="longdash") + 
  geom_text(aes(0, cost, label="initial cost", vjust=1.2, hjust=-1.5), color=2) + 
  labs(title="Monte Carlo simulation", x="Iterations", y="Empirical expected payoff") + 
  theme_minimal() + 
  transition_reveal(Iterations)
```

$~$

### 2 Metropolis Algorithm

Metropolis algorithm works as follows:

* Initialize a starting point as the current position (denoted by $\theta_\text{cur}$)
* Propose a $\theta_\text{pro}$ generated from a normal distribution with a mean of previous $\theta$ and a standard deviation of 0.2

  $$\theta_\text{pro}\sim\mathcal{N}(\theta_\text{cur}, 0.2^2)$$
  + We are modeling the probability distribution of the number of heads in a sequence of $N=20$ independent coin-flipping game with two possible outcomes (i.e. binomial distribution)
  + The parameter of interest ($\theta$) must be bounded between 0 and 1
  + Therefore, we refine the proposed distribution:

    $$
\theta_\text{pro}=
\begin{cases}
0, & \theta_\text{pro}<0 \\
\theta_\text{pro}, & 0<\theta_\text{pro}<1 \\
1, & \theta_\text{pro}>1 \\
\end{cases}$$

* Compute the prior probabilities of $\theta_\text{cur}$ and $\theta_\text{pro}$
  + Since we have a flat prior over $\theta$ (i.e. $\text{Beta}(1,1)$ is the beta form of uniform distribution), we can simply drop out the priors when computing the posteriors
* Compute the likelihood functions of $\theta_\text{cur}$ and $\theta_\text{pro}$ (binomial distribution)
* Compute the posterior probabilities of $\theta_\text{cur}$ and $\theta_\text{pro}$ (posterior is proportional to the likelihood function times the prior)
* Compute the acceptance ratio $p_\text{move}$

  $$\begin{aligned}
p_\text{move}
&= \min\left(1,
\frac{\text{posterior pribability of }\theta_\text{pro}}{\text{posterior pribability of }\theta_\text{cur}}
\right) \\
&= \min\left(1, \frac{\text{Binomial}(z,N\mid \theta_\text{pro})\cdot\text{Beta}(\theta_\text{pro}\mid a,b)}{\text{Binomial}(z,N\mid \theta_\text{cur})\cdot\text{Beta}(\theta_\text{cur}\mid a,b)} \right)
\end{aligned}$$
  + If $p_\text{move}\geq 0$, then accept the sample for sure by setting $\theta_\text{cur}$ to $\theta_\text{pro}$
  + If $p_\text{move}< 0$, then accept the sample with a probability of $p_\text{move}$. We proceeds by generating a uniform random number $u\in[0, 1]$. We accept the sample if $p_\text{move}>u$, otherwise we reject the sample by staying in the current island (setting $\theta_\text{cur}$ to $\theta_\text{cur}$)

$~$

#### 2(a) Implementation with R

```{r}
## Set seed for reproducible results
set.seed(2022)

## Function: chainGenerator()
## Arguments: start, n.samples, sigma, N, z, a, b
chainGenerator <- function(
    start,          # starting point
    n.samples=5000, # total number of samples generated
    sigma=0.2,      # standard deviation for the random draws
    N=20, z=14,     # parameters for the binomial distribution
    a=1, b=1        # parameters for the Beta distribution
  ) {
  chain <- numeric(n.samples) # initialize a vector to store samples
  chain[1] <- start           # set the first position of the vector to the starting point
  for (i in 1:(n.samples-1)) {
    theta.cur <- chain[i]
    theta.pro <- theta.cur + rnorm(1, mean=0, sd=sigma)
    theta.pro <- min(1, max(0, theta.pro))
    
    ## Posterior probability of the current theta
    posterior.cur <- dbinom(z, size=N, prob=theta.cur) * dbeta(theta.cur, a, b)
    
    ## Posterior probability of the proposed theta
    posterior.pro <- dbinom(z, size=N, prob=theta.pro) * dbeta(theta.pro, a, b)
    
    prob.move <- min(1, (posterior.pro/posterior.cur))
    
    ## Compare the probability of moving to a uniform draw from 0 to 1
    if (prob.move > runif(1, min=0, max=1)) {
      chain[i+1] <- theta.pro # accept the proposed sample 
    } else {
      chain[i+1] <- theta.cur # reject the proposed sample
    }
  }
  return(chain)
}
```

```{r}
## Generate three chains starting at 0.1, 0.9, 0.5 respectively
chain1 <- chainGenerator(start = 0.1)
chain2 <- chainGenerator(start = 0.9)
chain3 <- chainGenerator(start = 0.5)
```

$~$

#### 2(b) Overlaying three chains

```{r}
chains.r <- data.frame(
  ## Drop the first 2500 samples as burnins for each chain
  theta=c(chain1[2501:5000], chain2[2501:5000], chain3[2501:5000]), 
  iteration=rep(2501:5000, times=3), index=1:7500, 
  chainType=rep(c("chain1", "chain2", "chain3"), each=2500)
)
```

```{r, fig.show="hold", out.width="50%"}
ggplot(chains.r, aes(x=iteration, y=theta, col=chainType)) + 
  geom_line(alpha=0.5) + # geom_point() + 
  ggtitle("Trace plots of three chains") + 
  theme_minimal()

ggplot(chains.r, aes(x=theta, col=chainType, fill=chainType)) +
  geom_density(alpha=0.2) + 
  labs(title="Densities of three chains") +
  theme_minimal()
```

By overlaying trace plots of three chains, we can easily see that the three chains converge to the same posterior distribution. To see it more clear, we overlay densities of three chains and the densities overlap quite a bit.

$~$

#### 2(c) Concatenating three chains

```{r, fig.show="hold", out.width="50%"}
ggplot(chains.r, aes(x=index, y=theta, col=chainType)) + 
  geom_line(alpha=0.5) +  
  ggtitle("Trace plot of three chains concatenated") + 
  theme_minimal()

ggplot(chains.r, aes(x=theta)) +
  geom_density(alpha=0.2, color="darkblue", fill="lightblue") + 
  labs(title="Density plot of three chains concatenated") +
  theme_minimal()
```

```{r}
source("./scripts/HDIofMCMC.r")

hdi.r <- HDIofMCMC(chains.r$theta, credMass=0.95); hdi.r
```

```{r, fig.align="center"}
d.plot <- with(density(chains.r$theta), data.frame(x, y))
ggplot(data=d.plot, mapping=aes(x=x, y=y)) +
  geom_area(aes(x=ifelse(x>hdi.r[1] & x<hdi.r[2], x, 0), y=y), fill="lightblue", alpha=0.5) + 
  geom_line(color="darkblue") +
  geom_vline(xintercept=0.5, color="red") + ylim(0, 5) + theme_minimal() +
  geom_text(aes(0.5, 3, label="theta=0.5"), color="red") + 
  geom_text(aes(0.7, 2, label="95% HDI"), color="darkblue") + 
  labs(title="Density of theta with 95% HDI (Metropolis)", x="theta", y="density")
```

It is a bit difficult to visually tell from the density plot whether the red vertical line $\theta=0.5$ is within the 95% HDI but numerically we see that 0.5 is within the 95% HDI.

$~$

#### 2(d) Implementation with JAGS

```{r}
library(rjags)
library(runjags)
```

```{r}
## Null model
if (file.exists("./models/metropolis_model.txt")) {
  print("The text file for Metropolis model already existed!")
} else {
  cat("model{
      theta ~ dbeta(1, 1)    # (uniform) prior
      
      for (i in 1:Ntotal) {
        y[i] ~ dbern(theta)  # likelihood 
      }
  }", file="./models/metropolis_model.txt")
}
```

```{r}
n.flips <- 20 # the number of flips
n.heads <- 14 # the number of heads
y <- rep(c(1, 0), times=c(n.heads, n.flips-n.heads))
Ntotal <- length(y)

n.burnin <- 2500
n.samples <- 2500
n.thin <- 1

## Prepare the data for JAGS
dat <- dump.format(list(y=y, Ntotal=Ntotal))

## Initialize the variables and chains
inits1 <- dump.format(list(theta=0.1, .RNG.name="base::Super-Duper", .RNG.seed=99999 ))
inits2 <- dump.format(list(theta=0.9, .RNG.name="base::Wichmann-Hill", .RNG.seed=1234 ))
inits3 <- dump.format(list(theta=0.5, .RNG.name="base::Mersenne-Twister", .RNG.seed=6666 ))

## Tell JAGS which latent variables to monitor
monitor = c("theta")

## Run JAGS
results <- run.jags(
  model="./models/metropolis_model.txt", 
  monitor = monitor, 
  data = dat, 
  n.chains = 3, 
  inits = c(inits1, inits2, inits3), 
  burnin = n.burnin, 
  sample = n.samples, 
  thin = n.thin, 
  adapt = 0
)
```

```{r}
## Print the summary results of the Metropolis algorithm in a table
knitr::kable(summary(results), caption="Summary results of the Metropolis algorithm")
```

```{r, fig.align="center"}
chains.jags <- data.frame(
  chainType=rep(c("chain1", "chain2", "chain3"), each=2500), 
  index=rep(seq(2501, length.out=2500), times=3), 
  theta=rbind(results$mcmc[[1]], results$mcmc[[2]], results$mcmc[[3]])
)

ggplot(data=chains.jags) + 
  geom_line(aes(x=index, y=theta, color=chainType), alpha=0.5) + 
  labs(title="Trace of theta", x="Iterations") + 
  theme_minimal()
```

```{r}
hdi.jags <- HDIofMCMC(chains.jags$theta, credMass=0.95); hdi.jags
```

```{r, fig.align="center"}
d.plot <- with(density(chains.jags$theta), data.frame(x, y))
ggplot(data=d.plot, mapping=aes(x=x, y=y)) +
  geom_area(aes(x=ifelse(x>hdi.jags[1] & x<hdi.jags[2], x, 0), y=y), fill="lightblue", alpha=0.5) + 
  geom_line(color="darkblue") +
  geom_vline(xintercept=0.5, color="red") + 
  geom_text(aes(0.5, 3, label="theta=0.5"), color="red") + 
  geom_text(aes(0.7, 2, label="95% HDI"), color="darkblue") + 
  ylim(0, 5) + 
  labs(title="Density of theta with 95% HDI (Metropolis)", x="theta", y="density") + 
  theme_minimal()
```

We are able to visually detect that the red vertical line $\theta=0.5$ is within the 95% HDI (blue shaded density).

```{r, fig.align="center"}
d.plot <- data.frame(
  theta=c(chains.r$theta, chains.jags$theta), 
  method=rep(c("R", "JAGS"), each=7500)
)

ggplot(d.plot, aes(x=theta, color=method, fill=method)) + 
  geom_density(alpha=0.2) + theme_minimal() + 
  labs(title="Densities from R and JAGS")
```

```{r}
cat(sprintf("HDI from R is (%.4f, %.4f)\nHDI from JAGS is (%.4f, %.4f)", 
            hdi.r[1], hdi.r[2], hdi.jags[1], hdi.jags[2]))
```

Metropolis algorithm implementation with both R and JAGS leads to the same conclusion - the coin is fair, since we see that 0.5 falls into the highest density intervals generated by both R function and JAGS

$~$

### 3 IQ Test

```{r libraries, message=FALSE, warning=FALSE}
# Load the required libraries
library(rjags)
library(runjags)
source("./scripts/HDIofMCMC.r") # Load the HDI function
```

```{r, fig.align="center"}
## Read in the data
df <- read.csv("./scripts/TwoGroupIQ.csv")

ggplot(subset(df, subset=Group=="Smart Drug", select="Score"), aes(x=Score, y=..density..)) + 
  geom_histogram(bins=30, fill="#69b3a2", color="#e9ecef", alpha=0.5) + 
  ggtitle("Histogram of the IQ Score") +
  theme_minimal()
```

$~$

#### 3 (a) Expected IQ score > 100 ?

```{r}
## For purposes of this one-group example, use data from Smart Drug group:
IQscore <- df$Score[df$Group=="Smart Drug"]

## Save the length of the data and number of subjects (We want to pass this info to JAGS)
y <- IQscore
Ntotal <- length(y)

## Prepare the data for JAGS
dat <- dump.format(list(y=y, Ntotal=Ntotal))

## Initialize chains
inits1 <- dump.format(list(mu=100, sigma=30, .RNG.name="base::Super-Duper", .RNG.seed=99999 ))
inits2 <- dump.format(list(mu=120, sigma=20, .RNG.name="base::Wichmann-Hill", .RNG.seed=1234 ))
inits3 <- dump.format(list(mu=80, sigma=40, .RNG.name="base::Mersenne-Twister", .RNG.seed=6666 ))

## Tell JAGS which latent variables to monitor
monitor <- c("mu", "sigma")

## Run the function that fits the models using JAGS
results <- run.jags(
  model = "models/oneSampleNorm.txt",
  monitor = monitor, 
  data = dat, 
  n.chains = 3, 
  inits = c(inits1, inits2, inits3),
  burnin = 4000, # the number of burnin iterations
  sample = 1000, # the total number of samples to take
  thin = 4       # the thinning interval
)
```

```{r}
## Print the summary of the results in a table
knitr::kable(summary(results), caption="Summary results for the IS score")
```

```{r}
d.chains <- data.frame(
  chainType=rep(c("chain1", "chain2", "chain3"), each=1000), 
  index=rep(seq(5001, by=4, length.out=1000), times=3), 
  rbind(results$mcmc[[1]], results$mcmc[[2]], results$mcmc[[3]])
)

## Print the data frame
rmarkdown::paged_table(d.chains)
```

```{r}
## Trace plot of mu
trace.mu <- ggplot(data=d.chains) + 
  geom_line(aes(x=index, y=mu, color=chainType), alpha=0.5) + 
  labs(title="Trace of mu", x="Iterations") + 
  theme_minimal()

## Density plot of mu
density.mu <- ggplot(d.chains) + 
  geom_density(aes(x=mu), color="darkblue", fill="lightblue", alpha=0.5) + 
  ggtitle("Density of mu") + 
  theme_minimal()

## Trace plot of sigma
trace.sigma <- ggplot(data=d.chains) + 
  geom_line(aes(x=index, y=sigma, color=chainType), alpha=0.5) + 
  labs(title="Trace of sigma", x="Iterations") + 
  theme_minimal()

## Density plot of sigma
density.sigma <- ggplot(d.chains) + 
  geom_density(aes(x=sigma), color="darkblue", fill="lightblue", alpha=0.5) + 
  ggtitle("Density of sigma") + 
  theme_minimal()
```

```{r, fig.show="hold", out.width="50%"}
trace.mu; density.mu; trace.sigma; density.sigma
```

```{r}
## Highest density interval (HDI)
hdi.mu <- HDIofMCMC(d.chains$mu, credMass=0.95); hdi.mu
```

```{r, warning=FALSE, fig.align="center"}
d.plot <- with(density(d.chains$mu), data.frame(x, y))
ggplot(data=d.plot, mapping=aes(x=x, y=y)) +
  geom_area(aes(x=ifelse(x>hdi.mu[1] & x<hdi.mu[2], x, 0), y=y), fill="lightblue", alpha=0.5) +
  geom_area(aes(x=ifelse(x<100, x, 0), y=y), fill="red", alpha=0.5) +
  geom_line(color="darkblue") +
  xlim(90, 125) + 
  geom_vline(xintercept=100, color="red") +
  geom_vline(xintercept=median(d.chains$mu), color="darkblue") +
  geom_text(aes(98, 0.12, label="mu=100"), color="red") + 
  geom_text(aes(111, 0.12, label="mu=median"), color="darkblue") + 
  geom_text(aes(107, 0.04, label="95% HDI"), color="darkblue") + 
  labs(title="Density of mu (95% HDI)", x="mu", y="density") +
  theme_minimal()
```

* The shaded light blue area corresponds to the 95% highest density
* The shaded red area corresponds to the density probability of observing an IQ test with a value less than 100
* Visually it can be seen that these two shaded areas do not overlap. In other words, the 95% HDI does not cover 100. We thus conclude that the expected value of the IQ test is greater than 100

$$
\begin{aligned}
H_0: \quad & \mu\leq 100 \\
H_1: \quad & \mu>100
\end{aligned}
$$

```{r}
## Compute the empirical cumulative distribution function
Fn <- ecdf(d.chains$mu)

## Calculate P[mu < 100] from empirical CDF
p.value <- Fn(100); p.value

## Alternatively
p.value <- mean(d.chains$mu<100); p.value
```

* `ecdf()` function is used to compute the empirical cumulative distribution function and 100 is passed to the empirical CDF to calculate the $p$-value (i.e. $P(\mu<100)$)
* Alternatively, `mean()` function can also be used to compute the $p$-value since it returns the proportion of samples satisfying the condition
* $P(\mu<100)=0.009$. Therefore, we can conclude that the expected value of IQ score is statistically larger than 100 at the significance level of 0.009

$~$

#### 3 (b) Cohen's d

**Standardized mean difference:**

A (population) effect size $d$ based on means usually considers the standardized mean difference between two populations:
$$
d=\frac{\mu_1-\mu_2}{\sigma}
$$
where $\mu_1$ is the mean for one population, $\mu_2$ is the mean for the other population, and $\sigma$ is the standard deviation based on either or both population.

Note: this form for the effect size resembles the computation for a $t$-test statistic with the critical difference that the $t$-test statistic includes a factor of $\sqrt{n}$. This means that for a given effect size, the significance level increases with the sample size. Unlike the $t$-test statistic, the effect size aims to estimate a population parameter and is not affected by the sample size.

```{r}
cohen.d <- with(d.chains, (mu-100)/sigma)
mean(cohen.d)

hdi.cohend <- HDIofMCMC(cohen.d, credMass=0.95); hdi.cohend
```

```{r, fig.align="center"}
d.plot <- with(density(cohen.d), data.frame(x, y))
ggplot(data=d.plot, mapping=aes(x=x, y=y)) +
  geom_area(aes(x=ifelse(x>hdi.cohend[1] & x<hdi.cohend[2], x, 0), y=y), fill="lightblue", alpha=0.5) +
  geom_area(aes(x=ifelse(x<=0, x, 0), y=y), fill="red", alpha=0.5) +
  geom_line(color="darkblue") +
  ylim(0, 4) +
  geom_vline(xintercept=median(cohen.d), color="darkblue") +
  geom_vline(xintercept=0, color="red") +
  labs(title="Density of cohen's d (95% HDI)", x="cohen's d", y="density") +
  theme_minimal()
```

The plot displays the posterior distribution of the Cohen's *d*. The 95% highest density is denoted by the blue shaded area and the $P(d\leq0)$ is denoted by the red shaded area. We can visually tell that these two areas do not overlap, which indicates that the effect size is statistically larger than 0. We can also conduct a one-tailed test and get a numerical sense of the corresponding *p*-value.

$$
\begin{aligned}
H_0:\quad & d\leq 0 \\
H_1:\quad & d>0
\end{aligned}
$$

```{r}
## Compute the empirical cumulative distribution function
Fn <- ecdf(cohen.d)

## Calculate P[d<0] from empirical CDF
p.value <- Fn(0); p.value

## Alternatively
p.value <- mean(cohen.d < 0); p.value
```

The *p*-value is 0.009. Hence, we can reject the null hypothesis at the significance level of 0.09.

```{r}
d.table <- data.frame(
  EffectSize=c("Very small", "Small", "Medium", "Large", "Very large", "Huge"), 
  d=c(0.01, 0.20, 0.50, 0.80, 1.20, 2.0), 
  Reference=c("Sawilowsky, 2009", "Cohen, 1988", "Cohen, 1988", 
              "Cohen, 1988", "Sawilowsky, 2009", "Sawilowsky, 2009")
)
knitr::kable(d.table, caption="Effect Size Metric", align="c")
```

The Cohen's *d* is around 0.3. According to the effect size metric in the table above, we can conclude the effect size is between small and medium.

$~$

### 4 Model Comparison

```{r, message=FALSE, warning=FALSE}
## Load the libraries
library(runjags)
library(rjags)

library(ggplot2)
library(GGally)
```

$~$

#### 4(a) Visualization of raw data

```{r, fig.align="center"}
## Load the data
load("dataFit.RData")

## Pairs plot
df <- data.frame(x = x, y = y)
ggpairs(df, aes(x=x, y=y, color="Corr", alpha=0.2)) + 
  ggtitle("Pairs plot") + theme_minimal() + 
  scale_colour_manual(values = "red")
```

$~$

#### 4(b) Fitting different models

Null model:
$$
y=a
$$

Linear model:
$$
y=a+bx
$$

Quadratic model:
$$
y=a+bx+cx^2
$$

```{r}
## Null model
if (file.exists("./models/null_model.txt")) {
  print("The null model text file already existed!")
} else {
  cat("model{
      a ~ dnorm(0, 0.00001)        # prior for the intercept
      sigma ~ dunif(0.00001, 100)  # prior for the sd of y
      
      for (i in 1:Ntotal) {
        y[i] ~ dnorm(a, 1/(sigma^2))
      }
  }", file="./models/null_model.txt")
}

## Linear model
if (file.exists("./models/linear_model.txt")) {
  print("The linear model text file already existed!")
} else {
  cat("model{
      a ~ dnorm(0, 0.00001)        # prior for the intercept
      b ~ dnorm(0, 0.00001)        # prior for the slope
      sigma ~ dunif(0.00001, 100)  # prior for the sd of y
      
      for (i in 1:Ntotal) {
        mu[i] <- a + b * x[i]
        y[i] ~ dnorm(mu[i], 1/(sigma^2))
      }
  }", file="./models/linear_model.txt")
}

## Quadratic model
if (file.exists("./models/quadratic_model.txt")) {
  print("The quadratic model text file already existed!")
} else {
  cat("model{
      a ~ dnorm(0, 0.00001)        # prior for the intercept
      b ~ dnorm(0, 0.00001)        # prior for the slope
      c ~ dnorm(0, 0.00001)        # prior for the quadratic term
      sigma ~ dunif(0.00001, 100)  # prior for the sd of y
      
      for (i in 1:Ntotal) {
        mu[i] <- a + b * x[i] + c * x[i]^2
        y[i] ~ dnorm(mu[i], 1/(sigma^2))
      }
  }", file="./models/quadratic_model.txt")
}
```

```{r}
## Prepare the data for JAGS
dat <- dump.format(list(y=y, x=x, Ntotal=length(x)))

## Initialize each chain
inits1 <- dump.format(list(.RNG.name="base::Super-Duper", .RNG.seed=99999 ))
inits2 <- dump.format(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=1234 ))
inits3 <- dump.format(list(.RNG.name="base::Mersenne-Twister", .RNG.seed=6666 ))

## Tell JAGS which latent variables to monitor for each model
## We also need to monitor deviance to compute the dic
monitor0 <- c("a", "sigma", "deviance")
monitor1 <- c("a", "b", "sigma",  "deviance")
monitor2 <- c("a", "b", "c", "sigma", "deviance")

## Parameters for JAGS
n.samples <- 1000  # the number of samples to take
n.burnin  <- 5000  # the number of burnin iterations
n.thin    <- 5     # the thinning interval
```

```{r, warning=FALSE}
## Null model: y = a
if (exists("results0")) {
  print("Results for the null model already existed!")
} else {
  results0 <- run.jags(
    model = "models/null_model.txt", 
    monitor = monitor0, 
    data = dat, 
    n.chains = 3, 
    inits = c(inits1, inits2, inits3), 
    burnin = n.burnin, 
    sample = n.samples, 
    thin = n.thin
  )
}
```

```{r}
## Linear model: y = a + b*x
if (exists("results1")) {
  print("Results for the linear model already existed")
} else {
  results1 <- run.jags(
    model = "models/linear_model.txt", 
    monitor = monitor1, 
    data = dat, 
    n.chains = 3, 
    inits = c(inits1, inits2, inits3), 
    burnin = n.burnin, 
    sample = n.samples, 
    thin = n.thin
  )
}
```

```{r}
if (exists("results2")) {
  print("Results for the quadratic model already existed")
} else {
  results2 <- run.jags(
    model = "models/quadratic_model.txt", 
    monitor = monitor2, 
    data = dat, 
    n.chains = 3, 
    inits = c(inits1, inits2, inits3),
    burnin = n.burnin, 
    sample = n.samples, 
    thin = n.thin
  )
}
```

```{r}
## Summary results for null model
knitr::kable(summary(results0), caption="Summary results for the null model", digits=4)

## Summary results for linear model
knitr::kable(summary(results1), caption="Summary results for the linear model", digits=4)

## Summary results for quadratic model
knitr::kable(summary(results2), caption="Summary results for the quadratic model", digits=4)
```

$~$

#### 4(c) Visualization of data and predictions

```{r}
## Extract parameters from the summary results

## Null model: y = a0
a0 <- summary(results0)["a", "Mean"]

## Linear model: y = a1 + b1 * x
a1 <- summary(results1)["a", "Mean"]
b1 <- summary(results1)["b", "Mean"]

## Quadratic model: y = a2 + b2 * x + c2 * x^2
a2 <- summary(results2)["a", "Mean"]
b2 <- summary(results2)["b", "Mean"]
c2 <- summary(results2)["c", "Mean"]
```

```{r}
## Define the length of a grid of x-values
len <- 200

## Create a grid of x-values for prediction
x.grid <- seq(min(x), max(x), length.out=len)

## Predict y-values in the null model
y0.pred <- a0 + 0 * x.grid

## Predict y-values in the linear model
y1.pred <- a1 + b1 * x.grid

## Predict y-values in the quadratic model
y2.pred <- a2 + b2 * x.grid + c2 * x.grid^2
```

```{r, fig.align="center"}
d.plot <- data.frame(x=c(x.grid, x.grid, x.grid, x), 
                     y=c(y0.pred, y1.pred, y2.pred, y), 
                     type=rep(c("null", "linear", "quadratic", "data"), 
                              times=c(rep(len, 3), length(x))))

ggplot() +
  geom_line(data=d.plot[d.plot$type!="data", ], aes(x=x, y=y, color=type)) + 
  geom_point(data=d.plot[d.plot$type=="data", ], aes(x=x, y=y, color=type)) + 
  ggtitle("Plot of the real data and predictions of the three models") + 
  theme_minimal()
```

The figure displays three fits overlaid in the scatter plot of raw data. It is clear that the quadratic model fits the data the best against the other two.

$~$

#### 4(d) Deviance information certerion

$$
\texttt{DIC = mean(deviance) + (SD(deviance)^2)/2}
$$

```{r}
## Null model
m.dev0 <- summary(results0)["deviance", "Mean"] # mean of deviance in null model
sd.dev0 <- summary(results0)["deviance", "SD"]  # sd of deviance in null model
bic0 <- m.dev0 + (sd.dev0^2)/2                  # dic estimate in null model

## Linear model
m.dev1 <- summary(results1)["deviance", "Mean"] # mean of deviance in linear model
sd.dev1 <- summary(results1)["deviance", "SD"]  # sd of deviance in linear model
bic1 <- m.dev1 + (sd.dev1^2)/2                  # dic estimate in linear model

## Quadratic model
m.dev2 <- summary(results2)["deviance", "Mean"] # mean of deviance in quadratic model
sd.dev2 <- summary(results2)["deviance", "SD"]  # sd of deviance in quadratic model
bic2 <- m.dev2 + (sd.dev2^2)/2                  # dic estimate in quadratic model
```

```{r}
d.dic <- data.frame(cbind(bic0, bic1, bic2))
colnames(d.dic) <- c("Null Model", "Linear Model", "Quadratic Model")
rownames(d.dic) <- "DIC"
knitr::kable(d.dic, align="c", caption="DICs for three models")
```

The lower DIC indicates a better fit to the data. The table shows that the quadratic model yields the smallest DIC, thus the best fit out of three models, which is also consistent with what we see visually from the plot.