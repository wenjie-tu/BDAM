---
title: "Assignment 4"
subtitle: "Bayesian Data Analysis and Models of Behavior"
author: "Wenjie Tu"
date: "Spring Semester 2022"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang="us_en")
rm(list=ls())
```

$~$

### Part 1

$$
V_i(t)=V_i(t-1)+\eta
\underbrace{[\text{Reward}_i(t-1)-V_i(t-1)]}_\text{prediction error (PE)}
$$

* $V_i(t)$: the reward value expectation for the chosen option $i$ on trial $t$.
* $\eta$: the learning rate parameter.

The process of choosing between options can be described by the softmax choice rule:
$$
p_i(t)=\frac{\exp(\beta(t)\times V_i(t))}
{\sum_{j=1}^{n}\exp(\beta(t)\times V_j(t))}
$$

* $p_i(t)$: the probability that a decision maker will choose one option $i$ among all options $j$.
* $\beta$: the parameter that governs the sensitivity to rewards and the exploration-exploitation trade-off.

#### Summary table

```{r}
library(runjags)
library(coda)
library(rjags)
library(ggplot2)
library(HDInterval)
library(knitr)
```

```{r}
# Read in data
dat <- read.csv("./data/data.csv")

# Look at the structure of the data
str(dat)
```

```{r}
modelString <- "model {
  # Prior
  eta_g_mu ~ dunif(0, 1) # learning rate parameter
  eta_g_sig ~ dunif(0.001, 5)
  
  m_g_mu ~ dunif(1, 19) # drift rate saling parameter
  m_g_sig ~ dunif(0.001, 5)
  
  beta_g_mu ~ dunif(0, 1) # sensitivity parameter
  beta_g_sig ~ dunif(0.001, 5)
  
  # Transform sigma to precision
  eta_g_tau <- 1 / eta_g_sig^2
  m_g_tau <- 1 / m_g_sig^2
  beta_g_tau <- 1 / beta_g_sig^2
  
  # Group parameters
  for (g in 1:G) {
    # Dual learning rates
    for (v in 1:2) {
      eta_g[g, v] ~ dnorm(eta_g_mu, eta_g_tau)T(0, 1)
      eta_sig[g, v] ~ dunif(0.001, 5)
      eta_tau[g, v] <- 1 / eta_sig[g, v]^2
    }
    
    m_g[g] ~ dnorm(m_g_mu, m_g_tau)
    m_sig[g] ~ dunif(0.001, 5)
    m_tau[g] <- 1 / m_sig[g]^2
    
    beta_g[g] ~ dnorm(beta_g_mu, beta_g_tau)
    beta_sig[g] ~ dunif(0.001, 5)
    beta_tau[g] <- 1 / beta_sig[g]^2
  }
  
  # Subject parameters
  for (g in 1:G) {
    for (s in 1:S) {
      for (v in 1:2) {
        eta[g, s, v] ~ dnorm(eta_g[g, v], eta_tau[g, v])T(0, 1)
      }
      beta[g, s] ~ dnorm(beta_g[g], beta_tau[g])
      m[g, s] ~ dnorm(m_g[g], m_tau[g])
    }
  }
  
  # Loop over trials for each group and subject
  for (g in 1:G) {
    for (s in 1:S) {
      #Assign starting values to q[group,trial,stimulus pair,option].
      #'first' is a two-dimensional-array identifying first trial for each subject in each group.
      for (stim_pair in 1:3) {
        q[g, first[s, g], stim_pair, 1] <- 0
        q[g, first[s, g], stim_pair, 2] <- 0
      }
      # Run through trials
      # 'last' is a two-dimensional-array identifying last trial for each subject in each group.
      for (trial in (first[s, g]):(last[s, g]-1)) {
        # Calculate drift rate parameter as q-delta multiplied by m
        v[trial] <- ( q[g, trial, pair[trial], 1] - q[g, trial, pair[trial], 2] ) * m[g, s]
        
        pr[trial] <- ilogit(beta[g,s] + v[trial])
        rt_binary[trial] ~ dbern(pr[trial])
	      rt_predict[trial] ~ dbern(pr[trial])
	      
	      # Update q-values for next trial. 
        # 'pair' identifies the stimulus pair in the current trial 'not1' and 'not2' identifies the other stimulus pairs.
        q[g, trial+1, pair[trial], choice[trial]] <- q[g, trial, pair[trial], choice[trial]] + 
          eta[g, s, valence[trial]] * (value[trial] - q[g, trial, pair[trial], choice[trial]])
        
        q[g, trial+1, pair[trial], nonchoice[trial]] <- q[g, trial, pair[trial], nonchoice[trial]]
        q[g, trial+1, not1[trial], 1] <- q[g, trial, not1[trial], 1]
        q[g, trial+1, not1[trial], 2] <- q[g, trial, not1[trial], 2]
        q[g, trial+1, not2[trial], 1] <- q[g, trial, not2[trial], 1]
        q[g, trial+1, not2[trial], 2] <- q[g, trial, not2[trial], 2]
      }
      
      # Q-values are not updated in last trial
      for (trial in last[s, g]) {
        v[trial] <- (q[g, trial, pair[trial], 1] - q[g, trial, pair[trial], 2]) * m[g, s]
        
        # following the paper, beta is modeled using the power function and depends on t
        pr[trial] <- ilogit(beta[g, s] + v[trial])
        
        rt_binary[trial] ~ dbern(pr[trial])
	      rt_predict[trial] ~ dbern(pr[trial])
      }
    }
  }
}"

writeLines(modelString, con="./models/DriftDiffusionModel.txt")
```

```{r}
str(dat)
```
```{r}
dat$rt_binary <- ifelse(dat$rt >= 0, 1, 0)
niters <- 120 # max "trial" number for each subject (true max = 120)
dat <- dat[dat$iter <= niters, ]
nsubs <- 17   # max number of subjects (true max = 17)
dat <- dat[dat$ord_sbj <= nsubs, ]
dat$rownum <- 1:nrow(dat)

first <- aggregate(rownum ~ ord_sbj + med, data = dat, min)
first <- matrix(first$rownum, ncol = 2, nrow = nsubs)
last <- aggregate(rownum ~ ord_sbj + med, data = dat, max)
last <- matrix(last$rownum, ncol = 2, nrow = nsubs)

dat.jags <- dump.format(list(choice = dat$choice,
                             nonchoice = dat$nonchoice, 
                             value = dat$value, 
                             rt_binary = dat$rt_binary, 
                             pair = dat$pair, 
                             valence = dat$valence, 
                             S = length(unique(dat$ord_sbj)), 
                             first = as.matrix(first), 
                             last = as.matrix(last), 
                             not1 = dat$not_cond1, 
                             not2 = dat$not_cond2, 
                             G = 2))


inits1 <- dump.format(list(.RNG.name="base::Super-Duper", .RNG.seed=99999 ))
inits2 <- dump.format(list(.RNG.name="base::Wichmann-Hill", .RNG.seed=1234 ))
inits3 <- dump.format(list(.RNG.name="base::Mersenne-Twister", .RNG.seed=6666 ))

n.adapt <- 1000 
n.burnin <- 40000
n.sample <- 2000
n.thin <- 5

# Tell JAGS which latent variables to monitor
params <- c("eta_g", "eta", "m_g", "m", "beta_g", "beta") 
```

```{r}
# Run the function that fits the models using JAGS
start.time <- Sys.time()
results.ddm <- run.jags(model = "./models/DriftDiffusionModel.txt", 
                        monitor = params, 
                        data = dat.jags, 
                        inits = c(inits1, inits2, inits3), #, inits4),
                        plots = FALSE,
                        n.chains = 3, 
                        adapt = n.adapt, 
                        burnin = n.burnin, 
                        sample = n.sample, 
                        thin = n.thin, 
                        method = c("parallel"))
end.time <- Sys.time()
```

```{r}
end.time  - start.time
```

```{r}
kable(summary(results.ddm))
```

#### Histogram and density plots

```{r}
chains.ddm <- data.frame(rbind(results.ddm$mcmc[[1]], results.ddm$mcmc[[2]], results.ddm$mcmc[[3]]))

# Select columns for group-level learning rate parameters
chains.ddm <- chains.ddm[, 1:4]

column.names <- c("OFF-negative", "ON-negative", "OFF-positive", "ON-positive")
colnames(chains.ddm) <- column.names
```

```{r}
d.plot <- data.frame(
  eta_g = c(chains.ddm$`OFF-negative`, 
            chains.ddm$`ON-negative`, 
            chains.ddm$`OFF-positive`, 
            chains.ddm$`ON-positive`), 
  Group = rep(column.names, each=nrow(chains.ddm))
)
```

```{r}
ggplot(d.plot, aes(x=eta_g, y=..density.., color=Group, fill=Group)) + 
  geom_density(alpha=0.2) + 
  labs(title="Density plot of four group-level learning rate parameters", x=expression(eta[g]), y="Density") + 
  theme_classic()
```



#### Posterior density plots of difference

```{r}
ggplot(chains.ddm, aes(x=`ON-positive` - `OFF-positive`, y=..density..)) + 
  geom_density(color=3, fill=3, alpha=0.2) + 
  geom_vline(xintercept=hdi(chains.ddm$`ON-positive` - chains.ddm$`OFF-positive`), color=2, lty="longdash") + 
  labs(title="Positive learning rate (ON-OFF)", x=expression(eta^positive ~ ON-OFF), y="Density") + 
  theme_classic()
```

```{r}
ggplot(chains.ddm, aes(x=`ON-negative` - `OFF-negative`, y=..density..)) + 
  geom_density(color=4, fill=4, alpha=0.2) + 
  geom_vline(xintercept=hdi(chains.ddm$`ON-negative` - chains.ddm$`OFF-negative`), color=2, lty="longdash") +
  labs(title="Negative learning rate (ON-OFF)", x=expression(eta^positive ~ ON-OFF), y="Density") + 
  theme_classic()
```


#### Interpretations


$~$

### Part 2

#### Summary table

```{r}
model.sim <- extend.jags(results.ddm, 
                         drop.monitor = results.ddm$monitor, 
                         add.monitor = c("rt_predict"), 
                         sample = 1, 
                         adapt = 0, 
                         burnin = 0, 
                         summarise = FALSE, 
                         method = "parallel")
```

```{r}
rt.sim <- as.integer(model.sim$mcmc[[1]])
```

```{r}
newdat.jags <- dump.format(choice = dat$choice, 
                           nonchoice = dat$nonchoice, 
                           value = dat$value, 
                           rt_binary = rt.sim, 
                           pair = dat$pair, 
                           valence = dat$valence, 
                           S = length(unique(dat$ord_sbj)), 
                           first = as.matrix(first), 
                           last = as.matrix(last), 
                           not1 = dat$not_cond1, 
                           not2 = dat$not_cond2, 
                           G = 2)

n.adapt <- 1000 
n.burnin <- 40000 
n.sample <- 2000 
n.thin <- 1

# tell JAGS which latent variables to monitor
params <- c("etag", "eta",'eta_sd',"mg", "m", 'm_sd') 
```

```{r}
start.time <- Sys.time()
results <- run.jags(model = "./models/model.txt", 
                    monitor = params, 
                    data = newdat.jags, 
                    inits = c(inits1, inits2, inits3), 
                    plots = FALSE, 
                    n.chains = 3, 
                    adapt = n.adapt, 
                    burnin = n.burnin, 
                    sample = n.sample, 
                    thin = n.thin, 
                    method = c("parallel"))
end.time <- Sys.time()
```

```{r}
end.time - start.time
```

#### Posterior box plots


